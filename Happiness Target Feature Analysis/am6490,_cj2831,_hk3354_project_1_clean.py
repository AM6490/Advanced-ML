# -*- coding: utf-8 -*-
"""am6490, cj2831, hk3354 - Project_1_clean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wyl7Xswh02Z3eSacUJZ0IiUqVGWyKcIN

# Your Uni : am6490, cj2831, hk3354
# Your Full name : Arsh Misra, Conor Jones, Flora Kwon
# Link to your Public Github repository with Final report:
https://github.com/hyerhinkwon/QMSS5074-Adv-ML.git
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## 0. Loading Datasets"""

# Load the the World Happiness 2023 dataset
whr_df = pd.read_csv('https://raw.githubusercontent.com/hyerhinkwon/QMSS5074-Adv-ML/refs/heads/main/Project%201/WHR_2023.csv')

# Loading country data
countrydata=pd.read_csv('https://raw.githubusercontent.com/hyerhinkwon/QMSS5074-Adv-ML/refs/heads/main/Project%201/newcountryvars.csv')

"""Processing the World Happiness 2023 dataset."""

# Convert the regression target ('happiness_score') into classification labels
whr_df['happiness_category'] = pd.qcut(whr_df['happiness_score'],
                                       q=5,
                                       labels=['Very Low', 'Low','Average', 'High', 'Very High'])

"""Splitting training and test data."""

from sklearn.model_selection import train_test_split

# Select features and target
X = whr_df.drop(columns=['happiness_score', 'happiness_category'])
y = whr_df['happiness_category']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Convert y_train and y_test to numerical labels
y_train_labels = y_train.astype('category').cat.codes
y_test_labels = y_test.astype('category').cat.codes

# Reset indices
X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)

"""Merging the two data sets."""

# Merge in new data to X_train and X_test by taking "country" from first table and "country_name" from 2nd table.

# Check common countries.
X_train_common = set(X_train['country']).intersection(set(countrydata['country_name']))
print(X_train_common)
X_test_common = set(X_test['country']).intersection(set(countrydata['country_name']))
print(X_test_common)

# Merge
X_train = pd.merge(X_train, countrydata, left_on='country', right_on='country_name', how='left')
X_test = pd.merge(X_test, countrydata, left_on='country', right_on='country_name', how='left')

"""## 1.  EDA

Plotting the frequency distribution / histogram of some of the numerical features that we think are important.
"""

# Create the histogram
variables = ['population_below_poverty_line', 'perceptions_of_corruption', 'healthy_life_expectancy']
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
fig.suptitle('Histograms of Various Factors', fontsize=16)
for i, var in enumerate(variables):
    sns.histplot(data=X_train, x=var, kde=True, color='skyblue', edgecolor='black', ax=axes[i])
    axes[i].set_title(var.replace("_", " ").title())
    axes[i].set_xlabel('')

plt.tight_layout()
plt.show()

"""Plot the categorical variables and their distribution."""

# Your plotting code here:
cat_variables = ['region']
fig, axes = plt.subplots(1, 1, figsize=(10, 6))
fig.suptitle('Distribution of Regions', fontsize=16)
for i, var in enumerate(cat_variables):
    sns.countplot(data=X_train, x=var, ax=axes)
    axes.set_title(var.replace("_", " ").title())
    axes.set_xlabel('')
    axes.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

"""Perform feature correlation analysis to identify relationships between variables, using Use Pearson correlation coefficients to analyze feature dependencies."""

numerical_X_train = X_train.select_dtypes(include='float64')
numerical_X_train.corr(method='pearson')

"""Explore relationships between variables (bivariate, etc), correlation tables, and how they associate with the target variable."""

correlation_matrix = numerical_X_train.corr()
plt.figure(figsize=(8, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap of Features')
plt.show()

# How it relates to target feature.
fig, axes = plt.subplots(4, 4, figsize=(16, 16))
fig.suptitle('Scatter Plots of Numerical Variables vs Target Variable', fontsize=16)

axes = axes.flatten()

# Create scatter plots
num_plots = min(len(numerical_X_train.columns), len(axes))
for i, column in enumerate(numerical_X_train.columns[:num_plots]):
    sns.scatterplot(x=numerical_X_train[column], y=y_train, ax=axes[i])
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Target')
    axes[i].set_title(f'{column} vs Target')

for j in range(num_plots, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.show()

"""Also, detect outliers using box plots, Z-score analysis, or the IQR method to identify potential data anomalies."""

from scipy import stats

# Calculate Z-scores for each numerical X variable
z_scores = numerical_X_train.apply(stats.zscore)

# Identify potential outliers (Z-score > 3 or < -3)
outliers = (z_scores > 3) | (z_scores < -3)

# Print the number of outliers for each variable
print("Number of outliers per variable:")
print(outliers.sum())

"""## 2. Feature Engineering

Applying log transformations to normalize skewed data and improve model stability.
"""

# Your code here:

columns_to_log = ['gdp_per_capita', 'population', 'gni']
for col in columns_to_log:
    X_train[f'{col}_log'] = np.log1p(X_train[col])

# Visualize change
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Original vs Log-transformed Variables')

for i, col in enumerate(columns_to_log):
    axes[0, i].hist(X_train[col])
    axes[0, i].set_title(f'Original {col}')
    axes[1, i].hist(X_train[f'{col}_log'])
    axes[1, i].set_title(f'Log-transformed {col}')

plt.tight_layout()
plt.show()

"""Creating one interaction feature to capture relationship between existing variables, enhancing predictive power."""

X_train['freedom_healthy'] = X_train['freedom_to_make_life_choices'] * X_train['healthy_life_expectancy']

"""## 3.   Preprocess data using Sklearn Column Transformer/ Write and Save Preprocessor function"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Create the preprocessing pipelines for both numeric and categorical data.
numeric_features = X_train.select_dtypes(include=['float64'])
numeric_features=numeric_features.columns.tolist()

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_features = ['region', 'country', 'country_name']

# Replacing missing values with Modal value and then one hot encoding.
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Final preprocessor object set up with ColumnTransformer
preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),('cat', categorical_transformer, categorical_features)])

# Fit your preprocessor object
preprocess = preprocessor.fit(X_train)

# Transform data with preprocessor

def preprocessor(data):
    data.drop(['country', 'region'], axis=1)
    preprocessed_data=preprocess.transform(data)
    return preprocessed_data

"""## 4. Fit model on preprocessed data and save preprocessor function and model"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

model = RandomForestClassifier(random_state=42)

# Fit model
model.fit(preprocessor(X_train), y_train)

# Apply same log transformations and interaction variable to X_test
columns_to_log = ['gdp_per_capita', 'population', 'gni']
for col in columns_to_log:
    X_test[f'{col}_log'] = np.log1p(X_test[col])

X_test['freedom_healthy'] = X_test['freedom_to_make_life_choices'] * X_test['healthy_life_expectancy']


model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    max_features='sqrt',
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42)

model.fit(preprocessor(X_train), y_train)

"""##5. Generate predictions from X_test data and compare it with true labels in Y_test"""

# Score the model on testing data
test_score = model.score(preprocessor(X_test), y_test)
print(f"Testing Accuracy: {test_score:.4f}")

#-- Generate predicted values
prediction_labels = model.predict(preprocessor(X_test))

## Show model performance by comparing prediction_labels with true labels
accuracy = accuracy_score(y_test, prediction_labels)
print(f"Prediction Accuracy: {accuracy:.4f}")

"""##7. Basic Deep Learning"""

# Now experiment with deep learning models:
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation
from sklearn.preprocessing import LabelBinarizer
from keras.layers import Dropout, BatchNormalization

# Count features in input data
feature_count = preprocessor(X_train).shape[1]

num_classes = len(y_train.unique())

# Convert y_train to one-hot encoding
lb = LabelBinarizer()
y_train_encoded = lb.fit_transform(y_train)

# Use LeakyReLU activation
from keras.layers import LeakyReLU

leaky_relu = Sequential([
        Dense(128, input_dim=feature_count),
        LeakyReLU(alpha=0.1),
        Dense(64),
        LeakyReLU(alpha=0.1),
        Dense(64),
        LeakyReLU(alpha=0.1),
        Dense(32),
        LeakyReLU(alpha=0.1),
        Dense(num_classes, activation='softmax')
    ])

# Compile the model
leaky_relu.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

# Fitting the model to the Training set
history = leaky_relu.fit(preprocessor(X_train), y_train_encoded,
               batch_size = 20,
               epochs = 300, validation_split=0.25)


# Save history for plotting
history_dict = history.history

# Plot loss and accuracy at each epoch
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training', 'Validation'])

plt.figure()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Training', 'Validation'], loc='lower right')

"""##8. Explainability - SHAP Feature Importance

To better understand our model's predictions, we will use SHAP (SHapley Additive exPlanations) to analyze feature importance.
"""

# Import libraries
import shap
from sklearn.impute import SimpleImputer

# SHAP Analysis:
# Create a wrapper function to handle NaN values during prediction:
def predict_wrapper(X):
    predictions = leaky_relu.predict(X)
    # Handle potential NaN values in predictions (replace with a default value, e.g., 0)
    predictions = np.nan_to_num(predictions)
    return predictions

# Initialize SHAP explainer using the wrapper function
explainer = shap.KernelExplainer(predict_wrapper, preprocessor(X_train))

# Compute SHAP values for X_test
shap_values = explainer.shap_values(preprocess.transform(X_test))

# Generate SHAP summary plot
shap.summary_plot(shap_values, preprocess.transform(X_test), feature_names=X_train.columns)

"""<h3> Experimentation"""

# Define a Neural Network Model with 5 layers 128->64->64->32->5
keras_model = Sequential([
    Dense(128, input_dim=feature_count, activation='relu'), # Use the correct feature count
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(5, activation='softmax')
])

# Compile model
keras_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

# Convert y_train to one-hot encoding
lb = LabelBinarizer()
y_train_encoded = lb.fit_transform(y_train)

# Fitting the model to the Training set
history = keras_model.fit(preprocessor(X_train), y_train_encoded,
               batch_size = 20,
               epochs = 300, validation_split=0.25)

# Save history for plotting later
history_dict = history.history

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.inspection import permutation_importance

# Permutation Feature Importance Analysis with NaN Handling:

class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, keras_model, preprocessor):
        self.keras_model = keras_model
        self.preprocessor = preprocessor

    def fit(self, X, y):
        return self

# Get predicted class labels
    def predict(self, X):
        preprocessed_X = self.preprocessor(X)  # Call the function directly
        predictions = self.keras_model.predict(preprocessed_X)
        predictions = np.nan_to_num(predictions, nan=0.0)
        return predictions.argmax(axis=1)  # Return class labels

# Create an instance of the wrapper class
wrapper = KerasClassifierWrapper(keras_model, preprocessor)

# Calculate baseline accuracy
baseline_accuracy = accuracy_score(y_test_labels, wrapper.predict(X_test))  # Use predict_wrapper

# Perform permutation importance using the wrapper instance
result = permutation_importance(
    estimator=wrapper,  # Use the wrapper instance
    X=X_test,  # Pass the original X_test
    y=y_test_labels,
    n_repeats=30,
    random_state=42,
    scoring='accuracy'
)

# Process and Visualize Results:

# 1. Get Feature Importances and Sort
importances = result.importances_mean
sorted_idx = importances.argsort()

# 2. Create a DataFrame for easier handling
df_importances = pd.DataFrame({
    "Feature": X_train.columns[sorted_idx],
    "Importance": importances[sorted_idx]
})

# 3. Plotting
fig, ax = plt.subplots()
ax.barh(df_importances["Feature"], df_importances["Importance"])
ax.set_title("Permutation Feature Importance")
ax.set_xlabel("Importance")
plt.show()

# 4. Display the DataFrame
print(df_importances)

!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace your_notebook.ipynb

"""SHAP interaction provides shows how each variable contributes to predictions for individual instances. However, running the model took a long time, showing that it is extremely resource-intensive. We can visualize variable attributions but the the conceptual reasoning the model uses remains vague. In combination with permutation importance, we were able to identify relevant features globally to compare with the detailed per-prediction explanation from SHAP."""